{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bearer_token=\"AAAAAAAAAAAAAAAAAAAAABjFsgEAAAAAlpTaLAR1wJ2sut2HNk8oY2r9u28%3DRWpfy37hyg15PEYEQJAQwhjS9S3RasSkc1WV35KH1FhBPfAgjF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amy's API (24/3/2025 Night)\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAAqf0AEAAAAAF6IswZWwPiuk%2B09eVxEuSWz%2Bz4s%3Dt6rpSGfEfV8GPwkPDNSwuIF2R4n1M8H76mczQaQMRSidgKC2Wn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amy's API (24/3/2025 Morning)\n",
    "# bearer_token=\"AAAAAAAAAAAAAAAAAAAAAIuU0AEAAAAAru8T5RvIurrnOQpqTpoHoQlU0Us%3DUTuW3O0gtVB3Kq24bP3lInZcXklRdnwvChFWyut1C3kSZrQSli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yiwei's API?\n",
    "# bearer_token='AAAAAAAAAAAAAAAAAAAAAOF%2F0AEAAAAAK%2FOZioQs9h0%2FoETpHx6TgHVZ9%2Fs%3Dnuq6FzTc5XNfVyqjxFtD5EW6Kr34UwtXyVVC3oLKDIPaf27YWC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bearer_token='AAAAAAAAAAAAAAAAAAAAAOx80AEAAAAAJeXqsthToQ3e8feDsyg%2FUB%2Bc9%2F4%3DzSCfKQRUYIfOBgT88zGHCCMLPpwgfer84Vip8X7CI5d5WZJvh8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tw.Client(bearer_token=bearer_token, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_words = 'when life gives you tangerines'\n",
    "\n",
    "# response = client.search_recent_tweets(search_words, max_results=70)\n",
    "# tweets = response.data\n",
    "\n",
    "\n",
    "# collected_tweets = [tweet.text for tweet in tweets if not tweet.text.startswith(\"RT @\")]\n",
    "\n",
    "# df = pd.DataFrame(collected_tweets)\n",
    "# df.to_csv(\"tweets_tangerines.csv\", index=False)\n",
    "# print(f\"Saved {len(df)} tweets to tweets_tangerines.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_words = 'when life gives you tangerines lang:en'\n",
    "# max_results_per_request = 50  # Twitter allows max 100 per request\n",
    "# total_tweets_needed = 200  # Set how many tweets you want\n",
    "# collected_tweets = []  # Store fetched tweets\n",
    "# next_token = None  # Pagination token\n",
    "\n",
    "# # Fetch tweets in multiple requests\n",
    "# while len(collected_tweets) < total_tweets_needed:\n",
    "#     try:\n",
    "#         response = client.search_recent_tweets(\n",
    "#             query=search_words,\n",
    "#             max_results=max_results_per_request,\n",
    "#             tweet_fields=[\"created_at\", \"author_id\", \"lang\"],\n",
    "#             next_token=next_token  # Fetch next page\n",
    "#         )\n",
    "        \n",
    "#         if response.data:\n",
    "#             # Filter out retweets manually\n",
    "#             tweets = [{\"ID\": tweet.id, \"Timestamp\": tweet.created_at, \"User\": tweet.author_id, \"Tweet\": tweet.text} \n",
    "#                       for tweet in response.data if not tweet.text.startswith(\"RT @\") and tweet.lang == \"en\"] # manually filter retweets\n",
    "            \n",
    "#             collected_tweets.extend(tweets)\n",
    "#             print(f\"Collected {len(collected_tweets)} tweets so far...\")\n",
    "\n",
    "#         # Get next page token\n",
    "#         next_token = response.meta.get(\"next_token\")\n",
    "\n",
    "#         # Stop if no more tweets\n",
    "#         if not next_token:\n",
    "#             print(\"No more tweets available!\")\n",
    "#             break\n",
    "\n",
    "#         time.sleep(30)  # Avoid hitting rate limits\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         break\n",
    "\n",
    "# # Convert to DataFrame and Save to CSV\n",
    "# df = pd.DataFrame(collected_tweets)\n",
    "# df.to_csv(\"tweets_tangerines.csv\", index=False)\n",
    "# print(f\"Saved {len(df)} tweets to tweets_tangerines.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 6 tweets so far...\n",
      "Appended 6 new tweets to tweets_tangerines.csv!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 870 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "search_words = 'when life gives you tangerines lang:en'\n",
    "max_results_per_request = 50\n",
    "total_tweets_needed = 200\n",
    "collected_tweets = []\n",
    "next_token = None\n",
    "\n",
    "csv_filename = \"tweets_tangerines.csv\"\n",
    "\n",
    "# Fetch tweets in multiple requests\n",
    "while len(collected_tweets) < total_tweets_needed:\n",
    "    try:\n",
    "        response = client.search_recent_tweets(\n",
    "            query=search_words,\n",
    "            max_results=max_results_per_request,\n",
    "            tweet_fields=[\"created_at\", \"author_id\", \"lang\"],\n",
    "            next_token=next_token\n",
    "        )\n",
    "\n",
    "        if response.data:\n",
    "            # Filter out retweets and non-English tweets\n",
    "            tweets = [{\"ID\": tweet.id, \"Timestamp\": tweet.created_at, \"User\": tweet.author_id, \"Tweet\": tweet.text} \n",
    "                      for tweet in response.data if not tweet.text.startswith(\"RT @\") and tweet.lang == \"en\"]\n",
    "\n",
    "            collected_tweets.extend(tweets)\n",
    "            print(f\"Collected {len(collected_tweets)} tweets so far...\")\n",
    "\n",
    "            # Convert new tweets to DataFrame\n",
    "            df = pd.DataFrame(tweets)\n",
    "\n",
    "            # Append to CSV (without header if file exists)\n",
    "            df.to_csv(csv_filename, mode='a', index=False, header=not pd.io.common.file_exists(csv_filename))\n",
    "            print(f\"Appended {len(df)} new tweets to {csv_filename}!\")\n",
    "\n",
    "        # Get next page token\n",
    "        next_token = response.meta.get(\"next_token\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"No more tweets available!\")\n",
    "            break\n",
    "\n",
    "        time.sleep(30)  # Avoid hitting rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '\"when life gives you tangerines\" -filter:retweets  lang:en'  # Exclude retweets\n",
    "# max_tweets = 10000\n",
    "\n",
    "# # Fetch tweets\n",
    "# response = client.search_recent_tweets(query=query, max_results=max_tweets, tweet_fields=[\"created_at\", \"author_id\"])\n",
    "# tweets = response.data if response.data else []\n",
    "\n",
    "# # Store tweets in DataFrame\n",
    "# tweet_data = [{\"ID\": tweet.id, \"Timestamp\": tweet.created_at, \"User\": tweet.author_id, \"Tweet\": tweet.text} for tweet in tweets]\n",
    "# df = pd.DataFrame(tweet_data)\n",
    "\n",
    "# # Save to CSV to avoid unnecessary re-fetching\n",
    "# df.to_csv(\"tweets_tangerines.csv\", index=False)\n",
    "# print(f\"Saved {len(df)} tweets to tweets_tangerines.csv!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
