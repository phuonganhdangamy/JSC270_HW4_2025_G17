{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\phuon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\phuon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\phuon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import the Porter stemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Initial cleaning\n",
    "For later usage, skip this part and go to part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SKIP THIS PART IF DON'T WANT TO WAIT FOR THE DATA TO LOAD\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tw_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTwitter_Jan_Mar.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tw_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\phuon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phuon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phuon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\phuon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SKIP THIS PART IF DON'T WANT TO WAIT FOR THE DATA TO LOAD\n",
    "tw_data = pd.read_csv('Twitter_Jan_Mar.csv')\n",
    "tw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500036 entries, 0 to 500035\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   date           500036 non-null  object \n",
      " 1   id             500030 non-null  object \n",
      " 2   content        500030 non-null  object \n",
      " 3   username       500002 non-null  object \n",
      " 4   like_count     499974 non-null  float64\n",
      " 5   retweet_count  499974 non-null  float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 22.9+ MB\n"
     ]
    }
   ],
   "source": [
    "tw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date          0\n",
       "content       0\n",
       "like_count    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop missing labels\n",
    "tw_data = tw_data.dropna()\n",
    "\n",
    "# remove unused columns\n",
    "tw_data = tw_data.drop(columns=['id', 'username', 'retweet_count'])\n",
    "\n",
    "# check if there is any missing data left\n",
    "tw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from part 1 but made some changes on names/values\n",
    "\n",
    "# Create a new column in our DF that contains token lists instead of raw text\n",
    "def tokenize_text(df):\n",
    "  df['tokens'] =  df[\"content\"].apply(lambda x: x.split())\n",
    "\n",
    "# Remove any URL tokens\n",
    "def remove_url(df):\n",
    "  df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if not re.match(r'http\\S+', word)])\n",
    "\n",
    "# NEW: Remove any additional tokens related to chatgpt\n",
    "def remove_chatgpt(df):\n",
    "  chatgpt_terms = {'chatgpt', 'gpt', 'gpt-4', 'gpt4', 'gpt3', 'openai'}\n",
    "\n",
    "  df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word.lower() not in chatgpt_terms and \n",
    "                                                    not re.match(r'^[@#]\\S+', word)])\n",
    "\n",
    "\n",
    "# Remove all punctuation (,.?!;:â€™\") and special characters(@, #, +, &, =, $, etc). Also, convert all tokens to lowercase only. \n",
    "def add_cleaned_tokens(df):\n",
    "    cleaned_tokens = []\n",
    "    for row in df['tokens']:\n",
    "      cleaned_tokens.append([re.sub(r'[^a-zA-Z0-9]', '', t).lower() for t in row if re.sub(r'[^a-zA-Z0-9]', '', t)])\n",
    "    df['cleaned_tokens'] = cleaned_tokens\n",
    "\n",
    "# Stemm tokens by the Porter stememr\n",
    "def stem_tokens(df):\n",
    "  stemmer = PorterStemmer()\n",
    "  df['stemmed_tokens'] = df['cleaned_tokens'].apply(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
    "\n",
    "# Remove stopwords in english\n",
    "def remove_stopwords(df):\n",
    "  sw = stopwords.words('english')\n",
    "  tokens_no_sw = []\n",
    "  for row in df['stemmed_tokens']:\n",
    "    tokens_no_sw.append([t for t in row if t not in sw])\n",
    "  df['tokens_no_sw'] = tokens_no_sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>like_count</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>tokens_no_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-29 22:58:21+00:00</td>\n",
       "      <td>Free AI marketing and automation tools, strate...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Free, AI, marketing, and, automation, tools,,...</td>\n",
       "      <td>[free, ai, marketing, and, automation, tools, ...</td>\n",
       "      <td>[free, ai, market, and, autom, tool, strategi,...</td>\n",
       "      <td>[free, ai, market, autom, tool, strategi, coll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-29 22:58:18+00:00</td>\n",
       "      <td>@MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Chat, says, itâ€™s, 15., ðŸ˜‚]</td>\n",
       "      <td>[chat, says, its, 15]</td>\n",
       "      <td>[chat, say, it, 15]</td>\n",
       "      <td>[chat, say, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-29 22:57:53+00:00</td>\n",
       "      <td>https://t.co/FjJSprt0te - Chat with any PDF!\\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-, Chat, with, any, PDF!, Check, out, how, th...</td>\n",
       "      <td>[chat, with, any, pdf, check, out, how, this, ...</td>\n",
       "      <td>[chat, with, ani, pdf, check, out, how, thi, n...</td>\n",
       "      <td>[chat, ani, pdf, check, thi, new, ai, quickli,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-29 22:57:52+00:00</td>\n",
       "      <td>AI muses: \"In the court of life, we must all f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[AI, muses:, \"In, the, court, of, life,, we, m...</td>\n",
       "      <td>[ai, muses, in, the, court, of, life, we, must...</td>\n",
       "      <td>[ai, muse, in, the, court, of, life, we, must,...</td>\n",
       "      <td>[ai, muse, court, life, must, face, judg, dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-29 22:57:26+00:00</td>\n",
       "      <td>Most people haven't heard of Chat GPT yet.\\nFi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Most, people, haven't, heard, of, Chat, yet.,...</td>\n",
       "      <td>[most, people, havent, heard, of, chat, yet, f...</td>\n",
       "      <td>[most, peopl, havent, heard, of, chat, yet, fi...</td>\n",
       "      <td>[peopl, havent, heard, chat, yet, first, elit,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-03-29 22:58:21+00:00   \n",
       "1  2023-03-29 22:58:18+00:00   \n",
       "2  2023-03-29 22:57:53+00:00   \n",
       "3  2023-03-29 22:57:52+00:00   \n",
       "4  2023-03-29 22:57:26+00:00   \n",
       "\n",
       "                                             content  like_count  \\\n",
       "0  Free AI marketing and automation tools, strate...         0.0   \n",
       "1           @MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚         0.0   \n",
       "2  https://t.co/FjJSprt0te - Chat with any PDF!\\n...         0.0   \n",
       "3  AI muses: \"In the court of life, we must all f...         0.0   \n",
       "4  Most people haven't heard of Chat GPT yet.\\nFi...         0.0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Free, AI, marketing, and, automation, tools,,...   \n",
       "1                         [Chat, says, itâ€™s, 15., ðŸ˜‚]   \n",
       "2  [-, Chat, with, any, PDF!, Check, out, how, th...   \n",
       "3  [AI, muses:, \"In, the, court, of, life,, we, m...   \n",
       "4  [Most, people, haven't, heard, of, Chat, yet.,...   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [free, ai, marketing, and, automation, tools, ...   \n",
       "1                              [chat, says, its, 15]   \n",
       "2  [chat, with, any, pdf, check, out, how, this, ...   \n",
       "3  [ai, muses, in, the, court, of, life, we, must...   \n",
       "4  [most, people, havent, heard, of, chat, yet, f...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [free, ai, market, and, autom, tool, strategi,...   \n",
       "1                                [chat, say, it, 15]   \n",
       "2  [chat, with, ani, pdf, check, out, how, thi, n...   \n",
       "3  [ai, muse, in, the, court, of, life, we, must,...   \n",
       "4  [most, peopl, havent, heard, of, chat, yet, fi...   \n",
       "\n",
       "                                        tokens_no_sw  \n",
       "0  [free, ai, market, autom, tool, strategi, coll...  \n",
       "1                                    [chat, say, 15]  \n",
       "2  [chat, ani, pdf, check, thi, new, ai, quickli,...  \n",
       "3  [ai, muse, court, life, must, face, judg, dest...  \n",
       "4  [peopl, havent, heard, chat, yet, first, elit,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the data by going through all the steps from tokenization -> remove special characters -> stemming -> stopwords\n",
    "def process_data(df):\n",
    "  tokenize_text(df)\n",
    "  remove_url(df)\n",
    "  remove_chatgpt(df)\n",
    "  add_cleaned_tokens(df)\n",
    "  stem_tokens(df)\n",
    "  remove_stopwords(df)\n",
    "  return df\n",
    "\n",
    "tw_data = process_data(tw_data)\n",
    "\n",
    "tw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear unused column, renamed the final cleaned column\n",
    "tw_data['cleaned_content'] = tw_data['tokens_no_sw']\n",
    "tw_data = tw_data.drop(columns=['tokens', 'cleaned_tokens', 'stemmed_tokens', 'tokens_no_sw'])\n",
    "\n",
    "# convert to a csv file so that we don't have to run and wait raw data again\n",
    "tw_data.to_csv(\"twitter_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Vectorization and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>like_count</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-29 22:58:21+00:00</td>\n",
       "      <td>Free AI marketing and automation tools, strate...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['free', 'ai', 'market', 'autom', 'tool', 'str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-29 22:58:18+00:00</td>\n",
       "      <td>@MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['chat', 'say', '15']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-29 22:57:53+00:00</td>\n",
       "      <td>https://t.co/FjJSprt0te - Chat with any PDF!\\n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['chat', 'ani', 'pdf', 'check', 'thi', 'new', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-29 22:57:52+00:00</td>\n",
       "      <td>AI muses: \"In the court of life, we must all f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['ai', 'muse', 'court', 'life', 'must', 'face'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-29 22:57:26+00:00</td>\n",
       "      <td>Most people haven't heard of Chat GPT yet.\\nFi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['peopl', 'havent', 'heard', 'chat', 'yet', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499969</th>\n",
       "      <td>2023-01-04 07:18:08+00:00</td>\n",
       "      <td>@GoogleAI #LAMDA Versus @OpenAI #ChatGPT ?! Wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['versu', 'care', 'lamda', 'isnt', 'avail', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499970</th>\n",
       "      <td>2023-01-04 07:17:50+00:00</td>\n",
       "      <td>#ChatGPT \\n\\nSo much #Censorship.\\n\\nNever tru...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>['much', 'never', 'trust', 'system', 'dont', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499971</th>\n",
       "      <td>2023-01-04 07:17:20+00:00</td>\n",
       "      <td>all my twitter feed is about ChatGPT and @Open...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['twitter', 'feed', 'lol']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499972</th>\n",
       "      <td>2023-01-04 07:17:08+00:00</td>\n",
       "      <td>I'm quite amazed by Chat GPT. A really promisi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['im', 'quit', 'amaz', 'chat', 'gpt', 'realli'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499973</th>\n",
       "      <td>2023-01-04 07:16:56+00:00</td>\n",
       "      <td>I used chat gpt to get gym workout program and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['use', 'chat', 'get', 'gym', 'workout', 'prog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499974 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date  \\\n",
       "0       2023-03-29 22:58:21+00:00   \n",
       "1       2023-03-29 22:58:18+00:00   \n",
       "2       2023-03-29 22:57:53+00:00   \n",
       "3       2023-03-29 22:57:52+00:00   \n",
       "4       2023-03-29 22:57:26+00:00   \n",
       "...                           ...   \n",
       "499969  2023-01-04 07:18:08+00:00   \n",
       "499970  2023-01-04 07:17:50+00:00   \n",
       "499971  2023-01-04 07:17:20+00:00   \n",
       "499972  2023-01-04 07:17:08+00:00   \n",
       "499973  2023-01-04 07:16:56+00:00   \n",
       "\n",
       "                                                  content  like_count  \\\n",
       "0       Free AI marketing and automation tools, strate...         0.0   \n",
       "1                @MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚         0.0   \n",
       "2       https://t.co/FjJSprt0te - Chat with any PDF!\\n...         0.0   \n",
       "3       AI muses: \"In the court of life, we must all f...         0.0   \n",
       "4       Most people haven't heard of Chat GPT yet.\\nFi...         0.0   \n",
       "...                                                   ...         ...   \n",
       "499969  @GoogleAI #LAMDA Versus @OpenAI #ChatGPT ?! Wh...         1.0   \n",
       "499970  #ChatGPT \\n\\nSo much #Censorship.\\n\\nNever tru...         2.0   \n",
       "499971  all my twitter feed is about ChatGPT and @Open...         3.0   \n",
       "499972  I'm quite amazed by Chat GPT. A really promisi...         1.0   \n",
       "499973  I used chat gpt to get gym workout program and...         0.0   \n",
       "\n",
       "                                          cleaned_content  \n",
       "0       ['free', 'ai', 'market', 'autom', 'tool', 'str...  \n",
       "1                                   ['chat', 'say', '15']  \n",
       "2       ['chat', 'ani', 'pdf', 'check', 'thi', 'new', ...  \n",
       "3       ['ai', 'muse', 'court', 'life', 'must', 'face'...  \n",
       "4       ['peopl', 'havent', 'heard', 'chat', 'yet', 'f...  \n",
       "...                                                   ...  \n",
       "499969  ['versu', 'care', 'lamda', 'isnt', 'avail', 'r...  \n",
       "499970  ['much', 'never', 'trust', 'system', 'dont', '...  \n",
       "499971                         ['twitter', 'feed', 'lol']  \n",
       "499972  ['im', 'quit', 'amaz', 'chat', 'gpt', 'realli'...  \n",
       "499973  ['use', 'chat', 'get', 'gym', 'workout', 'prog...  \n",
       "\n",
       "[499974 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this instead of part (a) for future attempts\n",
    "tw_data = pd.read_csv('twitter_cleaned.csv')\n",
    "tw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def override_fcn(doc):\n",
    "  # We expect a list of tokens as input\n",
    "  return doc\n",
    "\n",
    "# Count Vectorizer\n",
    "def count_vectorizer(df):\n",
    "  X_df = df['cleaned_content'].to_numpy()\n",
    "  vocab_count = X_df.shape[0]\n",
    "\n",
    "  print(f\"The length of vocabulary is: {vocab_count}\")\n",
    "\n",
    "  count_vec = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer= override_fcn,\n",
    "    preprocessor= override_fcn,\n",
    "    token_pattern= None,\n",
    "    max_features = vocab_count)\n",
    "\n",
    "  counts_combined = count_vec.fit_transform(X_df)\n",
    "  counts = counts_combined[:len(X_df)]  # First part: Training data\n",
    "\n",
    "  print(f\"vec: {counts.toarray()}\")\n",
    "  print(f\"vec shape: {counts.shape}\")\n",
    "  return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of vocabulary is: 499974\n",
      "vec: [[ 9 20  9 ...  0  0  0]\n",
      " [ 2  6  2 ...  0  1  0]\n",
      " [15 32 15 ...  0  0  0]\n",
      " ...\n",
      " [ 2  6  2 ...  0  0  0]\n",
      " [10 22 10 ...  0  0  1]\n",
      " [10 22 10 ...  0  1  0]]\n",
      "vec shape: (499974, 41)\n"
     ]
    }
   ],
   "source": [
    "vec = count_vectorizer(tw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Vectorizing using TF-IDF vectors\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # TF-IDF Vectorizer\n",
    "# vectorizer = TfidfVectorizer(lowercase=True,\n",
    "#                                 #max_features=100,\n",
    "#                                 # max_df=0.99,\n",
    "#                                 # min_df=1,\n",
    "#                                 ngram_range = (1,3),\n",
    "#                                 stop_words = \"english\"\n",
    "#                             )\n",
    "\n",
    "# # fit the model to the tweet data\n",
    "# vectors = vectorizer.fit_transform(tw_data)\n",
    "\n",
    "# vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "def tfidf_vectorizer(counts):\n",
    "    tfidf_data = tfidf.fit_transform(counts)\n",
    "    print(f\"TF-IDF vec shape: {tfidf_data.shape}\")\n",
    "\n",
    "    return tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vec shape: (499974, 41)\n"
     ]
    }
   ],
   "source": [
    "# Apply TF-IDF Vectorizer to the train and test data\n",
    "tfidf_data = tfidf_vectorizer(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<499974x41 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10982348 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE CODE BELOW IS NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phuon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the tokenizer that just returns the tokens (as they are already tokenized)\n",
    "def override_fcn(doc):\n",
    "    return doc  # The doc is already tokenized\n",
    "\n",
    "# Assuming 'cleaned_content' is a column in your dataframe and contains tokenized words\n",
    "count_vec = CountVectorizer(\n",
    "    tokenizer=override_fcn,  # Use the pre-tokenized words as-is\n",
    "    preprocessor=None,  # No need for preprocessing as it's already cleaned\n",
    "    token_pattern=None,  # No token pattern, since tokens are already passed\n",
    "    stop_words='english',  # You can still remove stopwords if needed\n",
    "    ngram_range=(1, 3)  # Example: unigrams, bigrams, trigrams (you can adjust this)\n",
    ")\n",
    "\n",
    "# Transform the data into a count matrix\n",
    "counts_combined = count_vec.fit_transform(tw_data['cleaned_content'])  # Your tokenized column\n",
    "\n",
    "# Now apply TF-IDF transformation\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_data = tfidf.fit_transform(counts_combined)\n",
    "\n",
    "# Clustering\n",
    "k = 34  # Number of clusters\n",
    "\n",
    "# Fit the KMeans model\n",
    "model = KMeans(n_clusters=k, init=\"k-means++\", max_iter=300, n_init=1)\n",
    "model.fit(tfidf_data)\n",
    "\n",
    "# Sort the centroid coordinates in descending order (most important words in each cluster)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = count_vec.get_feature_names_out()  # Get the actual feature names (words)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"tweet_topic_chatgpt.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(k):\n",
    "        f.write(f\"Cluster {i}\\n\")\n",
    "        for ind in order_centroids[i, :10]:  # Top 10 words for each cluster\n",
    "            f.write(f\" {terms[ind]}\\n\")\n",
    "        f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# number of clusters\n",
    "k = 34\n",
    "\n",
    "# fit the k means model\n",
    "model = KMeans(n_clusters=k, init=\"k-means++\", max_iter=300, n_init=1)\n",
    "model.fit(tfidf_data)\n",
    "\n",
    "# sorts the centroid coordinates in descending order. most important words (highest weight in the cluster) come first\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "# write results to file\n",
    "with open (\"tweet_topic_chatgpt.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(k):\n",
    "        f.write(f\"Cluster {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write (' %s' % terms[ind],)\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
